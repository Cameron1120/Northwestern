{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ee0e15",
   "metadata": {},
   "source": [
    "## Classifying Cyberbullying Tweets using Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edc0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_text', 'cyberbullying_type'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47692 entries, 0 to 47691\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   tweet_text          47692 non-null  object\n",
      " 1   cyberbullying_type  47692 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 745.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#Load Cyberbullying Dataset \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#show all results\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "df1 = pd.read_csv('cyberbullying_tweets.csv')\n",
    "#df1 = df1.sample(frac=.1) #reduce file size and shuffle rows to retrieve all cyberbullying types back to test\n",
    "#df1=df1.drop(df1.index[20000:]) #reduces file size to test code\n",
    "print(df1.columns)\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb632843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12223e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cyberbullying_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d2d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert df1 cyberbullying label to .replace \n",
    "df1['cyberbullying_type'].replace(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying', 'age', 'ethnicity'],\n",
    "                        [0, 1, 2, 3,4,5], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d886c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "\n",
    "#append language \n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "\n",
    "df1['Lang'] = df1['tweet_text'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15026d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78478f73",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7e8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning of data\n",
    "\n",
    "#remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#lower case and punctuation removal \n",
    "import re, string \n",
    "\n",
    "def strip_all_entities(tweet_text): \n",
    "    tweet_text = tweet_text.replace('\\r', '').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    tweet_text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", tweet_text) #remove links and mentions\n",
    "    tweet_text = re.sub(r'[^\\x00-\\x7f]',r'', tweet_text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    tweet_text = tweet_text.translate(table)\n",
    "    tweet_text = [word for word in tweet_text.split() if word not in stop_words]\n",
    "    tweet_text = ' '.join(tweet_text)\n",
    "    tweet_text =' '.join(word for word in tweet_text.split() if len(word) < 14) # remove words longer than 14 characters\n",
    "    return tweet_text\n",
    "\n",
    "\n",
    "tweet_lst = []\n",
    "for t in df1.tweet_text:\n",
    "    tweet_lst.append(strip_all_entities(t)) \n",
    "\n",
    "\n",
    "df1 = df1.drop_duplicates() #drops duplicates \n",
    "\n",
    "df_en = df1[df1['Lang'] == 'en'] #drop none english tweets \n",
    "\n",
    "tweet_lst = df_en['tweet_text'].to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b774da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44605, 2500)\n"
     ]
    }
   ],
   "source": [
    "#tokenize. Preparing the data. https://www.kaggle.com/jonaspptawat/cyberbullying-classification-eda-and-ml#Data-Cleaning\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def stemmer(tweet_lst):\n",
    "    tokenized = nltk.word_tokenize(tweet_lst)\n",
    "    PS = PorterStemmer()\n",
    "    return ' '.join([PS.stem(words) for words in tokenized])\n",
    "\n",
    "#Lemmatization \n",
    "#NOTE:Stemming seems to work better for this dataset\n",
    "def lemmatize(tweet_lst):\n",
    "    tokenized = nltk.word_tokenize(tweet_lst)\n",
    "    lm = WordNetLemmatizer()\n",
    "    return ' '.join([lm.lemmatize(words) for words in tokenized])\n",
    "\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "\n",
    "x = cv.fit_transform(tweet_lst).toarray()\n",
    "y = df_en['cyberbullying_type'].to_list()\n",
    "\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285fb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yArray = np.array(y)\n",
    "yReshape = yArray.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd99aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(yReshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd15676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset, train and test sets     \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 14)\n",
    "x_trainS, x_testS, y_trainS, y_testS = train_test_split(x, onehot_encoded, test_size = 0.20, random_state = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933cdb32",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146a1f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6846\n",
      "Test set accuracy: 0.6319\n",
      "[[ 247   37   55  897   37   32]\n",
      " [ 109  759   58  478   54   81]\n",
      " [ 204   33 1249   22   47   78]\n",
      " [  81   34   15 1133   48   33]\n",
      " [ 101   28   19  423 1012   25]\n",
      " [  12    9   68   84   82 1237]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.19      0.24      1305\n",
      "           1       0.84      0.49      0.62      1539\n",
      "           2       0.85      0.76      0.81      1633\n",
      "           3       0.37      0.84      0.52      1344\n",
      "           4       0.79      0.63      0.70      1608\n",
      "           5       0.83      0.83      0.83      1492\n",
      "\n",
      "    accuracy                           0.63      8921\n",
      "   macro avg       0.67      0.62      0.62      8921\n",
      "weighted avg       0.69      0.63      0.63      8921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GN= GaussianNB()\n",
    "GN.fit(x_train, y_train)\n",
    "\n",
    "print (\"Training set accuracy: {:.4f}\". format(GN.score(x_train, y_train)))\n",
    "print (\"Test set accuracy: {:.4f}\". format(GN.score(x_test, y_test)))\n",
    "\n",
    "#confusion matrix \n",
    "y_pred= GN.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix= confusion_matrix (y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#precision and recall \n",
    "from sklearn.metrics import classification_report \n",
    "print (classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537e856",
   "metadata": {},
   "source": [
    "### K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541fa1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 761   49   10  470   12    3]\n",
      " [ 361  899    7  261    4    7]\n",
      " [ 544   66  680  324    7   12]\n",
      " [ 598   50    5  688    2    1]\n",
      " [ 188   14   27  121 1255    3]\n",
      " [ 187   21   33  153    8 1090]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.58      0.39      1305\n",
      "           1       0.82      0.58      0.68      1539\n",
      "           2       0.89      0.42      0.57      1633\n",
      "           3       0.34      0.51      0.41      1344\n",
      "           4       0.97      0.78      0.87      1608\n",
      "           5       0.98      0.73      0.84      1492\n",
      "\n",
      "    accuracy                           0.60      8921\n",
      "   macro avg       0.72      0.60      0.62      8921\n",
      "weighted avg       0.74      0.60      0.64      8921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#K-neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = classifier.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Precision/Recall\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d09cd",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93c8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text classification via TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=[2500], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "x_trainS=np.asarray(x_trainS)\n",
    "\n",
    "\n",
    "#model.save_weights('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47fe02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6701 - accuracy: 0.7605 - val_loss: 0.9484 - val_accuracy: 0.6402\n",
      "Epoch 2/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6766 - accuracy: 0.7599 - val_loss: 1.0672 - val_accuracy: 0.5965\n",
      "Epoch 3/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6654 - accuracy: 0.7631 - val_loss: 0.9493 - val_accuracy: 0.6453\n",
      "Epoch 4/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6714 - accuracy: 0.7634 - val_loss: 0.9111 - val_accuracy: 0.6405\n",
      "Epoch 5/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6679 - accuracy: 0.7648 - val_loss: 0.9607 - val_accuracy: 0.6363\n",
      "Epoch 6/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6677 - accuracy: 0.7624 - val_loss: 1.0234 - val_accuracy: 0.6080\n",
      "Epoch 7/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6652 - accuracy: 0.7632 - val_loss: 0.9247 - val_accuracy: 0.6598\n",
      "Epoch 8/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6576 - accuracy: 0.7670 - val_loss: 0.9759 - val_accuracy: 0.6352\n",
      "Epoch 9/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6601 - accuracy: 0.7670 - val_loss: 1.0038 - val_accuracy: 0.6400\n",
      "Epoch 10/10\n",
      "1004/1004 [==============================] - 5s 5ms/step - loss: 0.6576 - accuracy: 0.7652 - val_loss: 1.1437 - val_accuracy: 0.5873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17c7fd001c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_trainS, y_trainS,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c974369",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_predDecode = np.argmax(y_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03f97ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1097    2   56  150    0    0]\n",
      " [ 589  870    4   75    0    1]\n",
      " [ 109   10 1501   12    0    1]\n",
      " [1173    1    5  165    0    0]\n",
      " [1097    1    0  117  393    0]\n",
      " [  22   20    1  183    0 1266]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_predDecode)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d9a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
